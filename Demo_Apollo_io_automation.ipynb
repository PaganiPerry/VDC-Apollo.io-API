{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PaganiPerry/VDC-Apollo.io-API/blob/main/Demo_Apollo_io_automation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is a simple implemenation of Appolo.io API, which allows users to scrape contacts, companies, people, etc. from the Apollo.io website. While it's still a WIP, it does work with basic functionality. Currently, the main drawback is that it only retireives \"guessed\" emails and seems to activly avoid retrieving \"verified\" emails. This will be explored in a later version."
      ],
      "metadata": {
        "id": "XZvGMfqj26_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Imports the required libraries [run this cell first---you only need to run this cell once each run time]\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "dckzyozqdwnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title MAIN CELL [returns CSV File Mounted to Drive] RUN THIS\n",
        "\n",
        "\"\"\"Mount Google Drive\"\"\"\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\"\"\"Define the CSV file path in the shared folder \"VDC Engineering Contacts List\" within My Drive\"\"\"\n",
        "csv_file_path = \"/content/drive/My Drive/Apollo API Demo Folder/Apollo Demo.csv\" #Try changing this file name to \"My First Apollo Demo.csv\" and put it in a folder in My Drive, in your Drive, called \"My First Apollo API Demo Folder\" (hint, it might look something like whats below)\n",
        "                #\"/content/drive/My Drive/My First Apollo API Demo Folder/My First Apollo Demo\"\n",
        "\n",
        "\"\"\"Define the URL and API key\"\"\"\n",
        "url = \"https://api.apollo.io/v1/mixed_people/search\"\n",
        "mixed_people_search_api_key = 'ZuGNQ0UXawpNRxU_jxsbYQ'  # Your API key (don't share this with anyone outside the organization---not like they'd do much with it but don't do it)\n",
        "\n",
        "\"\"\"JUST RUN CELL FIRST and then it'll prompt you to enter in the link you have copied (previous version you had to manually change the static code below where domain was equal to the link)\"\"\"\n",
        "\"\"\" Get user input for domains or use predefined domains\"\"\"\n",
        "domain_input = input(\"Enter the domains separated by commas (or leave blank for default): \")\n",
        "\n",
        "if domain_input:\n",
        "    # Split input by commas and remove any extra whitespace\n",
        "    domains = [domain.strip() for domain in domain_input.split(\",\")]\n",
        "else:\n",
        "    # Default domains if no input is provided\n",
        "    domains = [\"http://www.vdcresearch.com/\"]  # You can modify this default list/value, but it has no real affect on what is printed\n",
        "\n",
        "\"\"\"Define titles to search for\"\"\"\n",
        "titles = [\"software engineer\", \"system architect\", \"software engineer in test\",\n",
        "          \"mechanical engineer\", \"product manager\", \"qa engineer\", \"software qa engineer\", \"electrical engineer\"] #Ex. if you want you can add or delete titles---like, \"strategic planner\" or \"quality assurance engineer\"\n",
        "          #Perhaps you could try adding\n",
        "\n",
        "headers = {\n",
        "    'Cache-Control': 'no-cache',\n",
        "    'Content-Type': 'application/json',\n",
        "    'X-Api-Key': mixed_people_search_api_key\n",
        "}\n",
        "\n",
        "\"\"\"Number of pages to fetch\"\"\"\n",
        "num_pages = 7 #Feel free to change this to however many---the number is, from what I can tell so far, rather arbitrary\n",
        "\n",
        "\"\"\"Load the existing CSV file (if it exists) to avoid duplicates and update missing data\"\"\"\n",
        "if os.path.isfile(csv_file_path):\n",
        "    existing_df = pd.read_csv(csv_file_path)\n",
        "    existing_contacts = existing_df[['Email']].drop_duplicates().set_index('Email')  # Set index to email for fast lookup\n",
        "else:\n",
        "    existing_df = pd.DataFrame()\n",
        "    existing_contacts = pd.DataFrame()  # Empty if no CSV exists\n",
        "\n",
        "data_list = []\n",
        "\n",
        "for domain in domains:\n",
        "    data = {\n",
        "        \"q_organization_domains\": domain,\n",
        "        \"per_page\": 100,\n",
        "        \"organization_num_employees_ranges\": [\"1,1000000\"],\n",
        "        \"person_titles\": titles,\n",
        "    }\n",
        "\n",
        "    for page in range(1, num_pages + 1):\n",
        "        data[\"page\"] = page\n",
        "        response = requests.post(url, headers=headers, json=data)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            result = response.json()\n",
        "            people = result.get('people', [])\n",
        "\n",
        "            if not people:\n",
        "                print(f\"No more results on page {page} for domain {domain}.\")\n",
        "                break\n",
        "\n",
        "            for person in people:\n",
        "                email = person.get('email')\n",
        "                if email and email != \"email_not_unlocked@domain.com\":\n",
        "                    title = person.get('title')\n",
        "                    first_name = person.get('first_name', '')\n",
        "                    last_name = person.get('last_name', '')\n",
        "                    company = person.get('organization', {}).get('name', '')\n",
        "\n",
        "                    if email not in existing_contacts.index:\n",
        "                        # New contact, add it\n",
        "                        data_list.append({\n",
        "                            'Title': title,\n",
        "                            'Email': email,\n",
        "                            'Domain': domain,\n",
        "                            'Company': company,\n",
        "                            'First Name': first_name,\n",
        "                            'Last Name': last_name\n",
        "                        })\n",
        "                    else:\n",
        "                        # If contact exists but has missing data, update it\n",
        "                        existing_contact = existing_df[existing_df['Email'] == email].iloc[0]\n",
        "\n",
        "                        # Check for missing fields and update them\n",
        "                        if pd.isna(existing_contact['Company']) or existing_contact['Company'] == '':\n",
        "                            existing_df.loc[existing_df['Email'] == email, 'Company'] = company\n",
        "                        if pd.isna(existing_contact['First Name']) or existing_contact['First Name'] == '':\n",
        "                            existing_df.loc[existing_df['Email'] == email, 'First Name'] = first_name\n",
        "                        if pd.isna(existing_contact['Last Name']) or existing_contact['Last Name'] == '':\n",
        "                            existing_df.loc[existing_df['Email'] == email, 'Last Name'] = last_name\n",
        "\n",
        "        else:\n",
        "            print(f\"Error on page {page} for domain {domain}: {response.status_code}, {response.text}\")\n",
        "            break\n",
        "\n",
        "\"\"\"Creates a DataFrame from the new data\"\"\"\n",
        "new_df = pd.DataFrame(data_list)\n",
        "\n",
        "\"\"\"Appends new data and update missing values for existing contacts\"\"\"\n",
        "if not new_df.empty:\n",
        "    combined_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
        "\n",
        "    \"\"\"Sorts by 'Domain' and save the updated CSV\"\"\"\n",
        "    combined_df.sort_values(by='Domain', ascending=True, inplace=True)\n",
        "    combined_df.to_csv(csv_file_path, index=False)\n",
        "    print(\"CSV updated with new and updated contacts.\")\n",
        "else:\n",
        "    \"\"\"Save any updated information in existing_df back to CSV\"\"\"\n",
        "    existing_df.to_csv(csv_file_path, index=False)\n",
        "    print(\"No new contacts found. Existing data updated where necessary.\")\n",
        "\n",
        "    #For the first times you try running the code, here are a few domains you could try using (or you could try you own of course):\n",
        "      #https://www.asus.com/us/, https://www.takeda.com, http://www.dhl.com/, http://www.huawei.com/, http://www.mi.com/, http://www.microsoft.com/, http://www.pepsico.com/, http://www.tsmc.com/, http://www.target.com/, http://www.bydglobal.com/"
      ],
      "metadata": {
        "id": "RvClYmDy_L60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title If the file/csv exists, the dataframe will be displayed below...\n",
        "\n",
        "\"\"\"Checks to see if the CSV exists. If it does, it'll read and then display it into a dataframe,\n",
        "and if it doesn't it'll simply display that it doesn't exist.\"\"\"\n",
        "\n",
        "# Define the CSV file path in the shared folder\n",
        "#                                   This portion below may need to be changed if shared, so that it fits the users/creators file and folder names\n",
        "csv_file = \"/content/drive/My Drive/Apollo API Demo Folder/Apollo Demo.csv\"\n",
        "\n",
        "\"\"\"Checks if the CSV file exists and read it into a DataFrame\"\"\"\n",
        "if os.path.isfile(csv_file):\n",
        "    df = pd.read_csv(csv_file)\n",
        "    print(\"Columns in DataFrame:\", df.columns)\n",
        "    display(df)\n",
        "\n",
        "    if 'Domain' in df.columns:\n",
        "        unique_domains = sorted(df['Domain'].unique())\n",
        "        print(\"\\nUnique domains already used (in alphabetical order):\")\n",
        "        for domain in unique_domains:\n",
        "            print(domain)\n",
        "    else:\n",
        "        print(\"The 'Domain' column is not in the DataFrame.\")\n",
        "else:\n",
        "    print(f\"File does not exist: {csv_file}\")\n"
      ],
      "metadata": {
        "id": "2ZtyG_-MR3f1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Downloads the most recent/updated file/csv\n",
        "\n",
        "\"\"\"Running this cell downloads the most recent version of Test Engineering Contact List to your LCOAL drive. \"\"\"\n",
        "\n",
        "# Define the CSV file path in the shared folder\n",
        "#                     This portion below may also need to be changed to match the owner/users file path\n",
        "csv_file = \"/content/drive/My Drive/Apollo API Demo Folder/Apollo Demo.csv\"\n",
        "#download the file\n",
        "from google.colab import files\n",
        "files.download(csv_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "RoJLqd3Fgbf-",
        "outputId": "8fa037c1-bae2-4beb-dbe9-64397f26bda4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a75d1232-a9da-436a-b887-a91a7c3e1bb0\", \"Apollo Demo.csv\", 10521)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  # @title GRAPH CATEGORIZED TITLES WITH GROUPING\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the CSV file path in the shared folder\n",
        "csv_file_path = \"/content/drive/My Drive/Apollo API Demo Folder/Apollo Demo.csv\"\n",
        "\n",
        "# Check if the CSV file exists and read it into a DataFrame\n",
        "if os.path.isfile(csv_file_path):\n",
        "    df = pd.read_csv(csv_file_path)\n",
        "\n",
        "    # Define a mapping of specific titles to broader categories\n",
        "    title_mapping = {\n",
        "        'software engineer': 'Software Engineering',\n",
        "        'software engineer in test': 'Software Engineering',\n",
        "        'qa engineer': 'Quality Assurance',\n",
        "        'software qa engineer': 'Quality Assurance',\n",
        "        'mechanical engineer': 'Engineering',\n",
        "        'electrical engineer': 'Engineering',\n",
        "        'product manager': 'Management',\n",
        "        'system architect': 'Architecture',\n",
        "        # Add more mappings as needed\n",
        "    }\n",
        "\n",
        "    # Replace titles with their categories\n",
        "    df['Category'] = df['Title'].replace(title_mapping)\n",
        "\n",
        "    # Count the occurrences of each category\n",
        "    category_counts = df['Category'].value_counts()\n",
        "\n",
        "    # Limit to top categories and group the rest as 'Other'\n",
        "    if len(category_counts) > 50:\n",
        "        top_categories = category_counts.nlargest(50).index\n",
        "        category_counts = category_counts[category_counts.index.isin(top_categories)]\n",
        "\n",
        "        # Count the 'Other' category\n",
        "        other_count = df['Category'][~df['Category'].isin(top_categories)].count()\n",
        "        category_counts = pd.concat([category_counts, pd.Series(other_count, index=['Other'])])\n",
        "\n",
        "    # Plot the most common categories\n",
        "    plt.figure(figsize=(20, 6))\n",
        "    category_counts.plot(kind='bar', color='lightcoral')\n",
        "    plt.title('Most Common Title Categories (Limited to a certain amount)')\n",
        "    plt.xlabel('Categories')\n",
        "    plt.ylabel('Count')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(f\"File does not exist: {csv_file_path}\")\n"
      ],
      "metadata": {
        "id": "O69urxfqGSnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Horizontal Copy & Paste Format [limited to 10 > GUESSED contacts---ORIGINAL CODE NO NEED TO TOUCH]\n",
        "\"\"\"Right now for some reason, it's only allowing users to scrap \"guessed\" contacts, which is a little frustrating. Hopefully it can be udpated in the future\"\"\"\n",
        "\n",
        "\n",
        "# API details\n",
        "url = \"https://api.apollo.io/v1/mixed_people/search\"\n",
        "api_key = 'ZuGNQ0UXawpNRxU_jxsbYQ'  # Your API key\n",
        "\n",
        "# Define parameters for the search\n",
        "data = {\n",
        "    \"q_organization_domains\": \"http://www.bp.com/\",  # Target company domain\n",
        "    \"per_page\": 100,  # Number of results per page\n",
        "    \"organization_num_employees_ranges\": [\"1,1000000\"],  # Filter by company size range\n",
        "    \"person_titles\": [\"software engineer\", \"system architect\", \"software engineer in test\", 'mechanical engineer', 'product manager', 'qa engineer', 'software qa engineer'],  # Search for titles\n",
        "    #\"contact_email_status\": [\"verified\"],  # Filter for verified emails only\n",
        "\n",
        "}\n",
        "\n",
        "headers = {\n",
        "    'Cache-Control': 'no-cache',\n",
        "    'Content-Type': 'application/json',\n",
        "    'X-Api-Key': api_key\n",
        "}\n",
        "\n",
        "# Number of pages to fetch\n",
        "num_pages = 3\n",
        "\n",
        "# Create a list to store each contact's title and email\n",
        "contact_list = []\n",
        "\n",
        "# Fetch data from the API\n",
        "for page in range(1, num_pages + 1):\n",
        "    # Update the page parameter\n",
        "    data[\"page\"] = page\n",
        "\n",
        "    response = requests.post(url, headers=headers, json=data)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        result = response.json()\n",
        "        people = result.get('people', [])\n",
        "\n",
        "        if not people:\n",
        "            print(f\"No more results on page {page}.\")\n",
        "            break\n",
        "\n",
        "        for person in people:\n",
        "            email = person.get('email')\n",
        "            # Filter out emails that are not unlocked\n",
        "            if email and email != \"email_not_unlocked@domain.com\":\n",
        "                title = person.get('title')\n",
        "                contact_list.append([title, email])\n",
        "                print(f\"Name: {person.get('name')}\")\n",
        "                print(f\"Title: {person.get('title')}\")\n",
        "                print(f\"Email: {email}\")\n",
        "                print()\n",
        "\n",
        "                # Stop after scraping data for 10 contacts (20 elements: 10 titles, 10 emails)\n",
        "                if len(contact_list) >= 10:\n",
        "                    break\n",
        "        if len(contact_list) >= 10:\n",
        "            break\n",
        "    else:\n",
        "        print(f\"Error on page {page}: {response.status_code}, {response.text}\")\n",
        "        break\n",
        "\n",
        "# Now create the MultiIndex DataFrame with blank columns in between contacts\n",
        "columns = []\n",
        "for i in range(1, len(contact_list) + 1):\n",
        "    columns.extend([(f'Contact {i}', 'Title'), (f'Contact {i}', 'Email'), ('', '')])\n",
        "\n",
        "# Remove the last blank column since it's not needed\n",
        "columns = columns[:-1]\n",
        "\n",
        "# Flatten the contact data into a list and insert NaN for blank columns\n",
        "flattened_contacts = []\n",
        "for contact in contact_list:\n",
        "    flattened_contacts.extend(contact)\n",
        "    flattened_contacts.append(np.nan)  # Add a blank space between contacts\n",
        "\n",
        "# Remove the last blank entry\n",
        "flattened_contacts = flattened_contacts[:-1]\n",
        "\n",
        "# Create the DataFrame with the contacts and blank columns\n",
        "df = pd.DataFrame([flattened_contacts], columns=pd.MultiIndex.from_tuples(columns))\n",
        "\n",
        "# Display the DataFrame\n",
        "df"
      ],
      "metadata": {
        "id": "k-agAtm364vJ",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}